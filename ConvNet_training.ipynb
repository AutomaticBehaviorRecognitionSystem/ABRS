{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) 2019 Primoz Ravbar UCSB\n",
    "# Licensed under BSD 2-Clause [see LICENSE for details]\n",
    "# Written by Primoz Ravbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load ST-images and labels for the ConvNet training\n",
    "\n",
    "import numpy as np\n",
    "import scipy\n",
    "from scipy import ndimage\n",
    "from scipy import misc\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import os\n",
    "import matplotlib.colors as mcolors\n",
    "import natsort\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, LSTM\n",
    "\n",
    "from ABRS_modules import discrete_radon_transform\n",
    "from ABRS_modules import etho2ethoAP\n",
    "from ABRS_modules import smooth_1d\n",
    "from ABRS_modules import create_LDA_training_dataset\n",
    "from ABRS_modules import removeZeroLabelsFromTrainingData\n",
    "from ABRS_modules import computeSpeedFromPosXY \n",
    "\n",
    "\n",
    "pathToABRSfolder = 'INSERT PATH TO ABRS MAIN FOLDER HERE'\n",
    "pathToABRSfolder = 'C:\\\\Users\\\\primo\\\\Desktop\\\\USB\\\\ABRS\\\\ABRS_Python_GHws1'\n",
    "\n",
    "#dirPathInput = 'INSERT PATH TO ABRS ST-images HERE';fileList = natsort.natsorted(os.listdir(dirPathInput))\n",
    "dirPathInput = 'C:\\\\Users\\\\primo\\\\Desktop\\\\USB\\\\data\\\\ST\\\\CommonFolder';fileList = natsort.natsorted(os.listdir(dirPathInput))\n",
    "\n",
    "dirPathLabel = pathToABRSfolder + '\\\\Labels';\n",
    "\n",
    "idxLabelDirPathFileName = dirPathLabel + '\\\\' + 'idxRecLabelLiAVI_manual_scoring'; #path to label file\n",
    "#idxLabelDirPathFileName = dirPathLabel + '\\\\' + 'labelCS1fb1_SS';\n",
    "\n",
    "outputFolderEtho = pathToABRSfolder + '\\\\Etho';\n",
    "\n",
    "\n",
    "with open(idxLabelDirPathFileName, \"rb\") as f:\n",
    "     idxLabel = pickle.load(f)\n",
    "\n",
    "labelShift = 10; # label onset correction\n",
    "\n",
    "shL = np.shape(idxLabel);\n",
    "labelShftRight = np.hstack((np.zeros((1,labelShift)),idxLabel[:,0:shL[1]-labelShift])); # works with janelia data 11/16/2018 # shift 15 works too\n",
    "idxLabel = labelShftRight;  \n",
    "idxLabel[idxLabel==0]=7\n",
    "\n",
    "numbFiles = np.shape(fileList)[0] #\n",
    "skipFilesNumb =1;\n",
    "skipFrameNumb=1;\n",
    "\n",
    "normalizeByMax = 1;\n",
    "thresholdMovement=250; #this is min. signal threshold (frames with no movement will not be used in training)\n",
    "\n",
    "\n",
    "yi = np.zeros((1,10))\n",
    "yiVect = np.zeros((1,1))\n",
    "\n",
    "rtImRec = np.zeros((50000,80,80,3))\n",
    "\n",
    "indIm = 0\n",
    "\n",
    "for fl in range(0, numbFiles-1, skipFilesNumb): #\n",
    "\n",
    "    inputFileName = fileList[fl];\n",
    "\n",
    "    fileDirPathInputName = dirPathInput + '\\\\' + inputFileName\n",
    "    \n",
    "    print(fileDirPathInputName)\n",
    "\n",
    "    with open(fileDirPathInputName, \"rb\") as f:\n",
    "        dict3C = pickle.load(f)\n",
    "        \n",
    "    recIm3C = dict3C[\"recIm3C\"]\n",
    "\n",
    "    maxMovRec = dict3C['maxMovementRec'];\n",
    "    labelFl = idxLabel[:, fl*50 : fl*50+50]\n",
    "    \n",
    "    \n",
    "    for i in range(0, recIm3C.shape[0]-1, skipFrameNumb):    \n",
    "                \n",
    "        im3CRaw = recIm3C[i,:,:,:]/1\n",
    "        \n",
    "        if np.count_nonzero(im3CRaw[:,:,0])>6400:            \n",
    "            im3CRaw[:,:,0] = np.zeros((80,80))\n",
    "        \n",
    "        if np.count_nonzero(im3CRaw[:,:,1])>800:            \n",
    "            im3CRaw[:,:,1] = np.zeros((80,80))\n",
    "        \n",
    "        rgbArray = np.zeros((80,80,3), 'uint8')\n",
    "        rgbArray[..., 0] = im3CRaw[:,:,0]\n",
    "        rgbArray[..., 1] = im3CRaw[:,:,1]\n",
    "        rgbArray[..., 2] = im3CRaw[:,:,2]\n",
    "        im3C = Image.fromarray(rgbArray)\n",
    "         \n",
    "\n",
    "        if fl == 0 and i == 0:\n",
    "    \n",
    "            rtImRec[indIm,:,:,:] = im3C\n",
    "            yi = np.zeros((1,10));\n",
    "            yi[0,int(labelFl[0,i])]=1\n",
    "            yRec = yi\n",
    "            yiVect = labelFl[0,i]\n",
    "            yVectRec = yiVect\n",
    "            \n",
    "            indIm=indIm+1\n",
    "            \n",
    "        if (fl > 0 or i > 0) and (maxMovRec[i] > thresholdMovement) and labelFl[0,i] != 7:\n",
    "            \n",
    "            imRandRotated = misc.imrotate(im3C,np.random.randint(360))\n",
    "            \n",
    "            rtImRec[indIm,:,:,:] = imRandRotated\n",
    "            \n",
    "            yi = np.zeros((1,10));\n",
    "            yi[0,int(labelFl[0,i])]=1            \n",
    "            yRec = np.vstack((yRec,yi))\n",
    "            yiVect = labelFl[0,i]\n",
    "            yVectRec = np.vstack((yVectRec,yiVect))\n",
    "            \n",
    "            indIm=indIm+1\n",
    "            \n",
    "            \n",
    "        #if maxMovRec[i] < thresholdMovement:\n",
    "         #   print(maxMovRec[i]);print('No movement detected')\n",
    "\n",
    "            \n",
    "Xin = rtImRec[0:indIm,:,:,:]\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\primo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "Train on 10698 samples, validate on 1189 samples\n",
      "Epoch 1/20\n",
      "10698/10698 [==============================] - 91s 9ms/sample - loss: 1.0595 - acc: 0.5860 - val_loss: 0.7159 - val_acc: 0.7199\n",
      "Epoch 2/20\n",
      "10698/10698 [==============================] - 73s 7ms/sample - loss: 0.6022 - acc: 0.7582 - val_loss: 0.5707 - val_acc: 0.7637\n",
      "Epoch 3/20\n",
      " 1888/10698 [====>.........................] - ETA: 56s - loss: 0.5008 - acc: 0.7945"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-028d9184f2e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     40\u001b[0m               metrics=['accuracy'])\n\u001b[0;32m     41\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myTrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'modelConv2ABRS_3C'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\primo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    878\u001b[0m           \u001b[0minitial_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m           \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 880\u001b[1;33m           validation_steps=validation_steps)\n\u001b[0m\u001b[0;32m    881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    882\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mc:\\users\\primo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[1;34m(model, inputs, targets, sample_weights, batch_size, epochs, verbose, callbacks, val_inputs, val_targets, val_sample_weights, shuffle, initial_epoch, steps_per_epoch, validation_steps, mode, validation_in_fit, **kwargs)\u001b[0m\n\u001b[0;32m    327\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m         \u001b[1;31m# Get outputs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 329\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    330\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m           \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\primo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3075\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[1;32m-> 3076\u001b[1;33m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[0;32m   3077\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3078\u001b[0m     return nest.pack_sequence_as(self._outputs_structure,\n",
      "\u001b[1;32mc:\\users\\primo\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#train with ConvNet\n",
    "\n",
    "y=yVectRec\n",
    "y=y[:,0]\n",
    "\n",
    "Xin = Xin/256 #normalize images to 0-1\n",
    "\n",
    "fShf, lShf = shuffle(Xin, y, random_state=0)\n",
    "XShf = fShf\n",
    "yShf = np.transpose(lShf)\n",
    "\n",
    "XTrain = XShf[0:int(np.shape(Xin)[0]/3),:,:,:] #use 1/3 of the images for training\n",
    "yTrain = yShf[0:int(np.shape(Xin)[0]/3)]\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, (5, 5), input_shape=Xin.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(16, (5, 5)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(16, (5, 5)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Flatten())  # \n",
    "\n",
    "model.add(Dense(128))\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "model.add(tf.keras.layers.Dense(10, activation=tf.nn.softmax))\n",
    "\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(XTrain, yTrain, batch_size=32, epochs=20, validation_split=0.1)\n",
    "\n",
    "model.save('modelConv2ABRS_3C') #save the graph and weights of the trained CNN to be used for classification\n",
    "\n",
    "\n",
    "##########################################################################\n",
    "\n",
    "predictionsProb = model.predict(Xin)\n",
    "\n",
    "predictionLabel = np.zeros((1,np.shape(predictionsProb)[0]))\n",
    "predictionLabel[0,:] = np.argmax(predictionsProb,axis=1) #this is the ethogram of the training data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
